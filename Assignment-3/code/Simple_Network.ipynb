{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4eae6115bdc54e1b983b8e2e7bd8360b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c34792ea47bb413cbb642efa2e6d875a",
              "IPY_MODEL_da2d85a3cd234f2db895bebc32148a02",
              "IPY_MODEL_2f49df9a5b4b49f89072f7eb8b1cce30"
            ],
            "layout": "IPY_MODEL_b896c25e0cce46a38dd1da658d039551"
          }
        },
        "c34792ea47bb413cbb642efa2e6d875a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9acc61d3cd37413a99933ab8336df3dc",
            "placeholder": "​",
            "style": "IPY_MODEL_0f46f194db174a4ea55ac3ae51958bf8",
            "value": "100%"
          }
        },
        "da2d85a3cd234f2db895bebc32148a02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c686ceaf507f4719a3f1c2971915f368",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_238a760708474b619a854c351425db4f",
            "value": 170498071
          }
        },
        "2f49df9a5b4b49f89072f7eb8b1cce30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6fec39f4b9f4557b1f39dd4bc24a1f7",
            "placeholder": "​",
            "style": "IPY_MODEL_93b54ca749914dacaf047e3b916ced77",
            "value": " 170498071/170498071 [00:02&lt;00:00, 71094115.91it/s]"
          }
        },
        "b896c25e0cce46a38dd1da658d039551": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9acc61d3cd37413a99933ab8336df3dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f46f194db174a4ea55ac3ae51958bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c686ceaf507f4719a3f1c2971915f368": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "238a760708474b619a854c351425db4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6fec39f4b9f4557b1f39dd4bc24a1f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93b54ca749914dacaf047e3b916ced77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676,
          "referenced_widgets": [
            "4eae6115bdc54e1b983b8e2e7bd8360b",
            "c34792ea47bb413cbb642efa2e6d875a",
            "da2d85a3cd234f2db895bebc32148a02",
            "2f49df9a5b4b49f89072f7eb8b1cce30",
            "b896c25e0cce46a38dd1da658d039551",
            "9acc61d3cd37413a99933ab8336df3dc",
            "0f46f194db174a4ea55ac3ae51958bf8",
            "c686ceaf507f4719a3f1c2971915f368",
            "238a760708474b619a854c351425db4f",
            "c6fec39f4b9f4557b1f39dd4bc24a1f7",
            "93b54ca749914dacaf047e3b916ced77"
          ]
        },
        "id": "lwshaQa8TOWJ",
        "outputId": "d81374a3-f6a7-428d-d1a7-03057dc42d89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4eae6115bdc54e1b983b8e2e7bd8360b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "23460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.6632930040359497 0.4369\n",
            "1 1.8011674880981445 0.5027\n",
            "2 1.1353694200515747 0.5224\n",
            "3 0.8839263319969177 0.5476\n",
            "4 1.250756859779358 0.562\n",
            "5 1.5115619897842407 0.5726\n",
            "6 0.8475539684295654 0.5728\n",
            "7 0.7453178763389587 0.5822\n",
            "8 1.3494120836257935 0.5959\n",
            "9 1.0869866609573364 0.5933\n",
            "10 1.1514432430267334 0.5979\n",
            "11 1.1242903470993042 0.5952\n",
            "12 1.0936717987060547 0.6039\n",
            "13 1.1802644729614258 0.6108\n",
            "14 0.9933774471282959 0.6054\n",
            "15 0.7920123934745789 0.6121\n",
            "16 0.523009181022644 0.6146\n",
            "17 0.8321347832679749 0.616\n",
            "18 0.9582446813583374 0.6116\n",
            "19 0.8899344801902771 0.614\n",
            "20 1.440187931060791 0.6169\n",
            "21 0.8439165949821472 0.6188\n",
            "22 0.9864885210990906 0.6162\n",
            "23 0.34797680377960205 0.6208\n",
            "24 1.3206325769424438 0.6197\n",
            "25 0.5868478417396545 0.6227\n",
            "26 0.7750813961029053 0.6233\n",
            "27 0.9410382509231567 0.6239\n",
            "28 0.9023637175559998 0.6185\n",
            "29 0.5876985788345337 0.6282\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "# Train using GPU. If not available on your machine, use google colab. \n",
        "import pandas as pd;\n",
        "from scipy.stats import zscore\n",
        "import torch as torch;\n",
        "import numpy as np\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn;\n",
        "import torch.nn.functional as F;\n",
        "np.random.seed(42)\n",
        "#train on GPU if available. It will take a long time to train if done on CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#read in the dataset\n",
        "num_classes=10;\n",
        "transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])])\n",
        "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, \n",
        "                             transform=transform )\n",
        "full_test_dataset = datasets.CIFAR10(root='./data', train=False, download=True,\n",
        "                             transform=transform )\n",
        "batch_size=64;\n",
        "trainloader = torch.utils.data.DataLoader(full_train_dataset, \n",
        "batch_size=batch_size,shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(full_test_dataset, \n",
        "batch_size=batch_size,shuffle=False)\n",
        "# create a neural network (inherit from nn.Module)\n",
        "class ConvNetWithBatchNorm(nn.Module):\n",
        "    # architecture of the network is specified in the constructor\n",
        "    def __init__(self): \n",
        "        super(ConvNetWithBatchNorm, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),         \n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),  \n",
        "            nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3),\n",
        "            nn.BatchNorm2d(num_features=12)           \n",
        "        )\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2)   \n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(12*6*6, 50),         \n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50,num_classes)            \n",
        "        )\n",
        "        \n",
        "    # here we specify the computation (forward phase of training) how \"x\" is transfered into output \"y\"\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.features1(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return F.log_softmax(x)\n",
        "    # constructor and forward() - that is all we need, the rest is implemented in the nn.Module and we inherit it\n",
        "# create an instance of the network\n",
        "model=ConvNetWithBatchNorm().to(device);\n",
        "criterion = F.nll_loss;\n",
        "# this optimizer will do gradient descent for us\n",
        "# experiment with learning rate and optimizer type\n",
        "learning_rate = 0.001;\n",
        "# note that we have to add all weights&biases, for both layers, to the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "# we add a learning rate scheduler, which will modify the learning rate during training\n",
        "# will initially start low, then increase it (\"warm up\"), and then gradually descrease it\n",
        "n_epochs = 30;\n",
        "num_updates = n_epochs*int(np.ceil(len(trainloader.dataset)/batch_size))\n",
        "print(num_updates)\n",
        "warmup_steps=1000;\n",
        "def warmup_linear(x):\n",
        "    if x < warmup_steps:\n",
        "        lr=x/warmup_steps\n",
        "    else:\n",
        "        lr=max( (num_updates - x ) / (num_updates - warmup_steps), 0.)\n",
        "    return lr;\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, warmup_linear);\n",
        "for i in range(n_epochs):\n",
        "    for j, data in enumerate(trainloader):\n",
        "      \n",
        "        inputs, labels = data        \n",
        "        inputs=inputs.to(device);\n",
        "        labels=labels.to(device);\n",
        "        \n",
        "        optimizer.zero_grad();\n",
        "        #forward phase - predictions by the model\n",
        "        outputs = model(inputs);\n",
        "        #forward phase - risk/loss for the predictions\n",
        "        risk = criterion(outputs, labels);\n",
        "  \n",
        "        # calculate gradients\n",
        "        risk.backward();\n",
        "        \n",
        "        # take the gradient step\n",
        "        optimizer.step();\n",
        "        scheduler.step();\n",
        "        \n",
        "        batch_risk=risk.item();\n",
        "    with (torch.no_grad()):\n",
        "      correct = 0;\n",
        "      for j, data in enumerate(testloader):\n",
        "        \n",
        "          inputs, labels = data        \n",
        "          inputs=inputs.to(device);\n",
        "          labels=labels.to(device);\n",
        "          outputs = model(inputs);\n",
        "          pred = outputs.data.max(dim=1, keepdim=True)[1]\n",
        "          correct += pred.eq(labels.data.view_as(pred)).sum().item();\n",
        "          \n",
        "    print(i, batch_risk, correct / len(testloader.dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot of the accuracy during epochs of training**"
      ],
      "metadata": {
        "id": "wG6Tqy_4Kf21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#save value of accuracy in list to plot graph:\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn;\n",
        "import torch.nn.functional as F;\n",
        "import torch as torch;\n",
        "from scipy.stats import zscore\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import transforms\n",
        "num_classes=10;\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "epochs=range(0,30)\n",
        "accuracy_list=[]\n",
        "a1=[]\n",
        "batch_size=64;\n",
        "transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])])\n",
        "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, \n",
        "                             transform=transform )\n",
        "full_test_dataset = datasets.CIFAR10(root='./data', train=False, download=True,\n",
        "                             transform=transform )\n",
        "batch_size=64;\n",
        "trainloader = torch.utils.data.DataLoader(full_train_dataset, \n",
        "batch_size=batch_size,shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(full_test_dataset, \n",
        "batch_size=batch_size,shuffle=False)\n",
        "class ConvNetWithBatchNorm(nn.Module):\n",
        "    # architecture of the network is specified in the constructor\n",
        "    def __init__(self): \n",
        "        super(ConvNetWithBatchNorm, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),         \n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),  \n",
        "            nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3),\n",
        "            nn.BatchNorm2d(num_features=12)           \n",
        "        )\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2)   \n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(12*6*6, 50),         \n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50,num_classes)            \n",
        "        )\n",
        "        \n",
        "    # here we specify the computation (forward phase of training) how \"x\" is transfered into output \"y\"\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.features1(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return F.log_softmax(x)\n",
        "    # constructor and forward() - that is all we need, the rest is implemented in the nn.Module and we inherit it\n",
        "# create an instance of the network\n",
        "model=ConvNetWithBatchNorm().to(device);\n",
        "criterion = F.nll_loss;\n",
        "# this optimizer will do gradient descent for us\n",
        "# experiment with learning rate and optimizer type\n",
        "learning_rate = 0.001;\n",
        "# note that we have to add all weights&biases, for both layers, to the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "# we add a learning rate scheduler, which will modify the learning rate during training\n",
        "# will initially start low, then increase it (\"warm up\"), and then gradually descrease it\n",
        "n_epochs = 30;\n",
        "num_updates = n_epochs*int(np.ceil(len(trainloader.dataset)/batch_size))\n",
        "print(num_updates)\n",
        "warmup_steps=1000;\n",
        "def warmup_linear(x):\n",
        "    if x < warmup_steps:\n",
        "        lr=x/warmup_steps\n",
        "    else:\n",
        "        lr=max( (num_updates - x ) / (num_updates - warmup_steps), 0.)\n",
        "    return lr;\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, warmup_linear);\n",
        "for i in range(n_epochs):\n",
        "    for j, data in enumerate(trainloader):\n",
        "      \n",
        "        inputs, labels = data        \n",
        "        inputs=inputs.to(device);\n",
        "        labels=labels.to(device);\n",
        "        \n",
        "        optimizer.zero_grad();\n",
        "        #forward phase - predictions by the model\n",
        "        outputs = model(inputs);\n",
        "        #forward phase - risk/loss for the predictions\n",
        "        risk = criterion(outputs, labels);\n",
        "  \n",
        "        # calculate gradients\n",
        "        risk.backward();\n",
        "        \n",
        "        # take the gradient step\n",
        "        optimizer.step();\n",
        "        scheduler.step();\n",
        "        \n",
        "        batch_risk=risk.item();\n",
        "    with (torch.no_grad()):\n",
        "      correct = 0;\n",
        "      for j, data in enumerate(testloader):\n",
        "        \n",
        "          inputs, labels = data        \n",
        "          inputs=inputs.to(device);\n",
        "          labels=labels.to(device);\n",
        "          outputs = model(inputs);\n",
        "          pred = outputs.data.max(dim=1, keepdim=True)[1]\n",
        "          correct += pred.eq(labels.data.view_as(pred)).sum().item();\n",
        "    accuracy = correct / len(testloader.dataset)\n",
        "    a1.append(accuracy)\n",
        "accuracy_list.append(a1)\n",
        "          \n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHINCCLBR383",
        "outputId": "8d636500-8c63-44b3-eb07-02e87bdb8373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "23460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-66e8caf16dc7>:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict1={}\n",
        "dict1[\"Simple_Network\"]=accuracy_list[0]\n",
        "#print(dict1)"
      ],
      "metadata": {
        "id": "qx1OrJr1UlcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "plt.plot(epochs,dict1[\"Simple_Network\"])\n",
        "plt.title('Simple_Network')\n",
        "plt.xlabel('Epochs', fontsize=16)\n",
        "plt.ylabel('Accuracy', fontsize=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Ibmv8FY5U1gy",
        "outputId": "95e3c225-9bf3-4555-c734-700ff369b3fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experimenting with different learning rate, keeping same number of epoch**"
      ],
      "metadata": {
        "id": "e33QtUazKjLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "# Train using GPU. If not available on your machine, use google colab. \n",
        "import pandas as pd;\n",
        "from scipy.stats import zscore\n",
        "import torch as torch;\n",
        "import numpy as np\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn;\n",
        "import torch.nn.functional as F;\n",
        "np.random.seed(42)\n",
        "#train on GPU if available. It will take a long time to train if done on CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#read in the dataset\n",
        "num_classes=10;\n",
        "transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])])\n",
        "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, \n",
        "                             transform=transform )\n",
        "full_test_dataset = datasets.CIFAR10(root='./data', train=False, download=True,\n",
        "                             transform=transform )\n",
        "batch_size=64;\n",
        "trainloader = torch.utils.data.DataLoader(full_train_dataset, \n",
        "batch_size=batch_size,shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(full_test_dataset, \n",
        "batch_size=batch_size,shuffle=False)\n",
        "# create a neural network (inherit from nn.Module)\n",
        "class ConvNetWithBatchNorm(nn.Module):\n",
        "    # architecture of the network is specified in the constructor\n",
        "    def __init__(self): \n",
        "        super(ConvNetWithBatchNorm, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),         \n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),  \n",
        "            nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3),\n",
        "            nn.BatchNorm2d(num_features=12)           \n",
        "        )\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2)   \n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(12*6*6, 50),         \n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50,num_classes)            \n",
        "        )\n",
        "        \n",
        "    # here we specify the computation (forward phase of training) how \"x\" is transfered into output \"y\"\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.features1(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return F.log_softmax(x)\n",
        "    # constructor and forward() - that is all we need, the rest is implemented in the nn.Module and we inherit it\n",
        "# create an instance of the network\n",
        "model=ConvNetWithBatchNorm().to(device);\n",
        "criterion = F.nll_loss;\n",
        "# this optimizer will do gradient descent for us\n",
        "# experiment with learning rate and optimizer type\n",
        "learning_rate = 0.0001;\n",
        "# note that we have to add all weights&biases, for both layers, to the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "# we add a learning rate scheduler, which will modify the learning rate during training\n",
        "# will initially start low, then increase it (\"warm up\"), and then gradually descrease it\n",
        "n_epochs = 30;\n",
        "num_updates = n_epochs*int(np.ceil(len(trainloader.dataset)/batch_size))\n",
        "print(num_updates)\n",
        "warmup_steps=1000;\n",
        "def warmup_linear(x):\n",
        "    if x < warmup_steps:\n",
        "        lr=x/warmup_steps\n",
        "    else:\n",
        "        lr=max( (num_updates - x ) / (num_updates - warmup_steps), 0.)\n",
        "    return lr;\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, warmup_linear);\n",
        "for i in range(n_epochs):\n",
        "    for j, data in enumerate(trainloader):\n",
        "      \n",
        "        inputs, labels = data        \n",
        "        inputs=inputs.to(device);\n",
        "        labels=labels.to(device);\n",
        "        \n",
        "        optimizer.zero_grad();\n",
        "        #forward phase - predictions by the model\n",
        "        outputs = model(inputs);\n",
        "        #forward phase - risk/loss for the predictions\n",
        "        risk = criterion(outputs, labels);\n",
        "  \n",
        "        # calculate gradients\n",
        "        risk.backward();\n",
        "        \n",
        "        # take the gradient step\n",
        "        optimizer.step();\n",
        "        scheduler.step();\n",
        "        \n",
        "        batch_risk=risk.item();\n",
        "    with (torch.no_grad()):\n",
        "      correct = 0;\n",
        "      for j, data in enumerate(testloader):\n",
        "        \n",
        "          inputs, labels = data        \n",
        "          inputs=inputs.to(device);\n",
        "          labels=labels.to(device);\n",
        "          outputs = model(inputs);\n",
        "          pred = outputs.data.max(dim=1, keepdim=True)[1]\n",
        "          correct += pred.eq(labels.data.view_as(pred)).sum().item();\n",
        "          \n",
        "    print(i, batch_risk, correct / len(testloader.dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YflIofn2KtVa",
        "outputId": "ae91310f-c041-4275-d8f6-2ca7a1991097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "23460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-c772b44ec47b>:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2.01163649559021 0.3101\n",
            "1 1.645772933959961 0.4087\n",
            "2 1.3056656122207642 0.445\n",
            "3 1.3651219606399536 0.4623\n",
            "4 0.9497140645980835 0.4833\n",
            "5 1.2678617238998413 0.4904\n",
            "6 1.2438929080963135 0.5019\n",
            "7 1.1489384174346924 0.5086\n",
            "8 1.2047457695007324 0.5133\n",
            "9 1.1149578094482422 0.5218\n",
            "10 1.0363249778747559 0.5274\n",
            "11 0.9563788771629333 0.5294\n",
            "12 1.5888748168945312 0.5334\n",
            "13 1.2980492115020752 0.5393\n",
            "14 0.8553378582000732 0.5393\n",
            "15 1.820970058441162 0.5476\n",
            "16 0.9582483172416687 0.5483\n",
            "17 1.814244270324707 0.5509\n",
            "18 1.444668173789978 0.5557\n",
            "19 0.9110224843025208 0.5468\n",
            "20 1.444419026374817 0.5523\n",
            "21 1.525260090827942 0.556\n",
            "22 1.0589969158172607 0.5635\n",
            "23 1.0464682579040527 0.5597\n",
            "24 1.564648151397705 0.5572\n",
            "25 1.0338785648345947 0.5578\n",
            "26 0.6884133815765381 0.5627\n",
            "27 0.9485715627670288 0.5601\n",
            "28 1.8122165203094482 0.5627\n",
            "29 1.539196252822876 0.559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "# Train using GPU. If not available on your machine, use google colab. \n",
        "import pandas as pd;\n",
        "from scipy.stats import zscore\n",
        "import torch as torch;\n",
        "import numpy as np\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn;\n",
        "import torch.nn.functional as F;\n",
        "np.random.seed(42)\n",
        "#train on GPU if available. It will take a long time to train if done on CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#read in the dataset\n",
        "num_classes=10;\n",
        "transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])])\n",
        "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, \n",
        "                             transform=transform )\n",
        "full_test_dataset = datasets.CIFAR10(root='./data', train=False, download=True,\n",
        "                             transform=transform )\n",
        "batch_size=64;\n",
        "trainloader = torch.utils.data.DataLoader(full_train_dataset, \n",
        "batch_size=batch_size,shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(full_test_dataset, \n",
        "batch_size=batch_size,shuffle=False)\n",
        "# create a neural network (inherit from nn.Module)\n",
        "class ConvNetWithBatchNorm(nn.Module):\n",
        "    # architecture of the network is specified in the constructor\n",
        "    def __init__(self): \n",
        "        super(ConvNetWithBatchNorm, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),         \n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),  \n",
        "            nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3),\n",
        "            nn.BatchNorm2d(num_features=12)           \n",
        "        )\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2)   \n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(12*6*6, 50),         \n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50,num_classes)            \n",
        "        )\n",
        "        \n",
        "    # here we specify the computation (forward phase of training) how \"x\" is transfered into output \"y\"\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.features1(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return F.log_softmax(x)\n",
        "    # constructor and forward() - that is all we need, the rest is implemented in the nn.Module and we inherit it\n",
        "# create an instance of the network\n",
        "model=ConvNetWithBatchNorm().to(device);\n",
        "criterion = F.nll_loss;\n",
        "# this optimizer will do gradient descent for us\n",
        "# experiment with learning rate and optimizer type\n",
        "learning_rate = 0.00001;\n",
        "# note that we have to add all weights&biases, for both layers, to the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "# we add a learning rate scheduler, which will modify the learning rate during training\n",
        "# will initially start low, then increase it (\"warm up\"), and then gradually descrease it\n",
        "n_epochs = 30;\n",
        "num_updates = n_epochs*int(np.ceil(len(trainloader.dataset)/batch_size))\n",
        "print(num_updates)\n",
        "warmup_steps=1000;\n",
        "def warmup_linear(x):\n",
        "    if x < warmup_steps:\n",
        "        lr=x/warmup_steps\n",
        "    else:\n",
        "        lr=max( (num_updates - x ) / (num_updates - warmup_steps), 0.)\n",
        "    return lr;\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, warmup_linear);\n",
        "for i in range(n_epochs):\n",
        "    for j, data in enumerate(trainloader):\n",
        "      \n",
        "        inputs, labels = data        \n",
        "        inputs=inputs.to(device);\n",
        "        labels=labels.to(device);\n",
        "        \n",
        "        optimizer.zero_grad();\n",
        "        #forward phase - predictions by the model\n",
        "        outputs = model(inputs);\n",
        "        #forward phase - risk/loss for the predictions\n",
        "        risk = criterion(outputs, labels);\n",
        "  \n",
        "        # calculate gradients\n",
        "        risk.backward();\n",
        "        \n",
        "        # take the gradient step\n",
        "        optimizer.step();\n",
        "        scheduler.step();\n",
        "        \n",
        "        batch_risk=risk.item();\n",
        "    with (torch.no_grad()):\n",
        "      correct = 0;\n",
        "      for j, data in enumerate(testloader):\n",
        "        \n",
        "          inputs, labels = data        \n",
        "          inputs=inputs.to(device);\n",
        "          labels=labels.to(device);\n",
        "          outputs = model(inputs);\n",
        "          pred = outputs.data.max(dim=1, keepdim=True)[1]\n",
        "          correct += pred.eq(labels.data.view_as(pred)).sum().item();\n",
        "          \n",
        "    print(i, batch_risk, correct / len(testloader.dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEodyqAVObJH",
        "outputId": "3377679f-5f48-4998-a210-a747fc4d85b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "23460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-606e79e2fc51>:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2.2458057403564453 0.1314\n",
            "1 2.2231431007385254 0.2102\n",
            "2 2.052966833114624 0.2569\n",
            "3 2.1734440326690674 0.2831\n",
            "4 1.8281080722808838 0.3067\n",
            "5 2.0180130004882812 0.3197\n",
            "6 1.7664457559585571 0.3405\n",
            "7 2.1939525604248047 0.3456\n",
            "8 2.0565967559814453 0.36\n",
            "9 2.173945426940918 0.3652\n",
            "10 1.833620548248291 0.3635\n",
            "11 1.7990665435791016 0.3719\n",
            "12 1.9646238088607788 0.3774\n",
            "13 1.5115443468093872 0.3819\n",
            "14 1.754597544670105 0.3861\n",
            "15 1.5276862382888794 0.3948\n",
            "16 1.517659068107605 0.3916\n",
            "17 1.467589259147644 0.3895\n",
            "18 1.8288536071777344 0.3975\n",
            "19 1.203397274017334 0.402\n",
            "20 1.6628414392471313 0.3978\n",
            "21 1.5560632944107056 0.4058\n",
            "22 1.6484302282333374 0.4065\n",
            "23 1.637061595916748 0.4009\n",
            "24 1.545371651649475 0.405\n",
            "25 1.750301480293274 0.4055\n",
            "26 1.3462525606155396 0.408\n",
            "27 1.6410033702850342 0.4069\n",
            "28 1.7382957935333252 0.4033\n",
            "29 1.647829532623291 0.4092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "# Train using GPU. If not available on your machine, use google colab. \n",
        "import pandas as pd;\n",
        "from scipy.stats import zscore\n",
        "import torch as torch;\n",
        "import numpy as np\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn;\n",
        "import torch.nn.functional as F;\n",
        "np.random.seed(42)\n",
        "#train on GPU if available. It will take a long time to train if done on CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#read in the dataset\n",
        "num_classes=10;\n",
        "transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])])\n",
        "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, \n",
        "                             transform=transform )\n",
        "full_test_dataset = datasets.CIFAR10(root='./data', train=False, download=True,\n",
        "                             transform=transform )\n",
        "batch_size=64;\n",
        "trainloader = torch.utils.data.DataLoader(full_train_dataset, \n",
        "batch_size=batch_size,shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(full_test_dataset, \n",
        "batch_size=batch_size,shuffle=False)\n",
        "# create a neural network (inherit from nn.Module)\n",
        "class ConvNetWithBatchNorm(nn.Module):\n",
        "    # architecture of the network is specified in the constructor\n",
        "    def __init__(self): \n",
        "        super(ConvNetWithBatchNorm, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),         \n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),  \n",
        "            nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3),\n",
        "            nn.BatchNorm2d(num_features=12)           \n",
        "        )\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2)   \n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(12*6*6, 50),         \n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50,num_classes)            \n",
        "        )\n",
        "        \n",
        "    # here we specify the computation (forward phase of training) how \"x\" is transfered into output \"y\"\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.features1(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return F.log_softmax(x)\n",
        "    # constructor and forward() - that is all we need, the rest is implemented in the nn.Module and we inherit it\n",
        "# create an instance of the network\n",
        "model=ConvNetWithBatchNorm().to(device);\n",
        "criterion = F.nll_loss;\n",
        "# this optimizer will do gradient descent for us\n",
        "# experiment with learning rate and optimizer type\n",
        "learning_rate = 0.01;\n",
        "# note that we have to add all weights&biases, for both layers, to the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "# we add a learning rate scheduler, which will modify the learning rate during training\n",
        "# will initially start low, then increase it (\"warm up\"), and then gradually descrease it\n",
        "n_epochs = 30;\n",
        "num_updates = n_epochs*int(np.ceil(len(trainloader.dataset)/batch_size))\n",
        "print(num_updates)\n",
        "warmup_steps=1000;\n",
        "def warmup_linear(x):\n",
        "    if x < warmup_steps:\n",
        "        lr=x/warmup_steps\n",
        "    else:\n",
        "        lr=max( (num_updates - x ) / (num_updates - warmup_steps), 0.)\n",
        "    return lr;\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, warmup_linear);\n",
        "for i in range(n_epochs):\n",
        "    for j, data in enumerate(trainloader):\n",
        "      \n",
        "        inputs, labels = data        \n",
        "        inputs=inputs.to(device);\n",
        "        labels=labels.to(device);\n",
        "        \n",
        "        optimizer.zero_grad();\n",
        "        #forward phase - predictions by the model\n",
        "        outputs = model(inputs);\n",
        "        #forward phase - risk/loss for the predictions\n",
        "        risk = criterion(outputs, labels);\n",
        "  \n",
        "        # calculate gradients\n",
        "        risk.backward();\n",
        "        \n",
        "        # take the gradient step\n",
        "        optimizer.step();\n",
        "        scheduler.step();\n",
        "        \n",
        "        batch_risk=risk.item();\n",
        "    with (torch.no_grad()):\n",
        "      correct = 0;\n",
        "      for j, data in enumerate(testloader):\n",
        "        \n",
        "          inputs, labels = data        \n",
        "          inputs=inputs.to(device);\n",
        "          labels=labels.to(device);\n",
        "          outputs = model(inputs);\n",
        "          pred = outputs.data.max(dim=1, keepdim=True)[1]\n",
        "          correct += pred.eq(labels.data.view_as(pred)).sum().item();\n",
        "          \n",
        "    print(i, batch_risk, correct / len(testloader.dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPM9xENSS5Ko",
        "outputId": "0a543466-3dad-4165-94b2-86cadaf8b11f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "23460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-c0f4f3649dc8>:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.4112709760665894 0.4633\n",
            "1 0.8432642817497253 0.5149\n",
            "2 1.5822584629058838 0.5346\n",
            "3 1.5187464952468872 0.5512\n",
            "4 1.620132327079773 0.5508\n",
            "5 1.0071877241134644 0.5637\n",
            "6 0.9873939752578735 0.5749\n",
            "7 1.0510880947113037 0.5777\n",
            "8 1.3073549270629883 0.578\n",
            "9 1.202437162399292 0.5857\n",
            "10 1.1965711116790771 0.5865\n",
            "11 1.178345799446106 0.5862\n",
            "12 1.3560312986373901 0.59\n",
            "13 0.7493314743041992 0.5809\n",
            "14 1.513296365737915 0.5898\n",
            "15 1.500539779663086 0.5984\n",
            "16 1.2150651216506958 0.5959\n",
            "17 1.036895751953125 0.6011\n",
            "18 1.0451395511627197 0.5964\n",
            "19 1.3561651706695557 0.6038\n",
            "20 1.6175291538238525 0.6018\n",
            "21 0.9216997623443604 0.597\n",
            "22 0.6686849594116211 0.604\n",
            "23 0.7766129374504089 0.5994\n",
            "24 0.8195895552635193 0.5991\n",
            "25 0.74049973487854 0.6119\n",
            "26 0.7605166435241699 0.6026\n",
            "27 0.66623455286026 0.605\n",
            "28 1.7994211912155151 0.6031\n",
            "29 1.079270601272583 0.6036\n"
          ]
        }
      ]
    }
  ]
}